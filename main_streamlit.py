import streamlit as st
import pandas as pd
import math
import re
from src.ingestion import read_file
from src.geocoding import geocode_dataframe, clean_address

# Configuration
st.set_page_config(page_title="Robot de G√©ocodage", layout="wide")
st.title("üìç Robot de G√©ocodage")

# Initialisation des √©tats
if "df" not in st.session_state:
    st.session_state.df = None

if "last_selected_enriched_df" not in st.session_state:
    st.session_state.last_selected_enriched_df = None

if "enriched_df" not in st.session_state:
    st.session_state.enriched_df = None

if "cleaned_df" not in st.session_state:
    st.session_state.cleaned_df = None

if "batch_results" not in st.session_state:
    st.session_state.batch_results = []

if "modified_rows" not in st.session_state:
    st.session_state.modified_rows = set()

# ========== UPLOAD ET LECTURE ==========
st.subheader("üìÅ Chargement du fichier")

uploaded_file = st.file_uploader("D√©pose un fichier CSV ou TXT", type=["csv", "txt"])

# Gestion de la r√©initialisation lorsque le fichier est retir√©
if uploaded_file is None:
    st.session_state.df = None
    st.session_state.enriched_df = None
    st.session_state.cleaned_df = None
    st.session_state.batch_results = []
    st.session_state.modified_rows = set()
    st.session_state.mapping_config = {
        "fields": {},
        "attribute_selected": None
    }

# D√©tection de changement de fichier pour reset automatique
current_filename = uploaded_file.name if uploaded_file else None
previous_filename = st.session_state.get("previous_filename", None)

if uploaded_file and current_filename != previous_filename:
    # R√©initialiser tous les √©tats li√©s au pr√©c√©dent fichier
    st.session_state.df = None
    st.session_state.enriched_df = None
    st.session_state.cleaned_df = None
    st.session_state.batch_results = []
    st.session_state.modified_rows = set()
    st.session_state.mapping_config = {
        "fields": {},
        "attribute_selected": None
    }
    st.session_state.previous_filename = current_filename  # Sauvegarder le nouveau nom

if uploaded_file and st.session_state.df is None:
    df = read_file(uploaded_file, sep=None)

    if not df.empty:
        st.session_state.df = df
    else:
        st.error("‚ùå Erreur de lecture du fichier.")

if st.session_state.df is not None:
    st.success("‚úÖ Fichier charg√© avec succ√®s !")
    st.dataframe(st.session_state.df.head())

# ========== MAPPING ==========
df = st.session_state.df

if df is not None:
    st.subheader("üß© Mapping des Colonnes")

    possible_fields = ['name', 'street', 'postal_code', 'city', 'governorate', 'country', 'complement']

    if "mapping_config" not in st.session_state:
        st.session_state.mapping_config = {
            "fields": {}
        }

    mapping_config = st.session_state.mapping_config
    current_mapping = {}

    for field in possible_fields:
        current_mapping[field] = st.selectbox(
            f"Colonne pour {field}",
            options=["-- Aucun --"] + list(df.columns),
            index=(["-- Aucun --"] + list(df.columns)).index(mapping_config["fields"].get(field, "-- Aucun --"))
            if "fields" in mapping_config and field in mapping_config["fields"] else 0,
            key=f"mapping_{field}"
        )

    mapped_fields = {k: v for k, v in current_mapping.items() if v != "-- Aucun --"}
    st.session_state.mapping_config["fields"] = mapped_fields

    # G√©n√©ration automatique de la colonne 'full_address'
    if mapped_fields:
        full_address_parts = []

        if "name" in mapped_fields:
            full_address_parts.append(df[mapped_fields["name"]].astype(str))

        for key in ['street', 'postal_code', 'city', 'governorate', 'country', 'complement']:
            if key in mapped_fields:
                full_address_parts.append(df[mapped_fields[key]].astype(str))

        if full_address_parts:
            df["full_address"] = full_address_parts[0]
            for part in full_address_parts[1:]:
                df["full_address"] += ", " + part

            st.success("‚úÖ Colonne 'full_address' g√©n√©r√©e automatiquement !")
            #st.dataframe(df.head())
            st.dataframe(st.session_state.df.head())
        else:
            st.warning("‚ö†Ô∏è Aucun champ mapp√© pour g√©n√©rer 'full_address'.")

# ========== G√âOCODAGE PAR BATCH ==========
if st.session_state.df is not None and "full_address" in st.session_state.df.columns:
    st.subheader("üìç G√©ocodage Multi-API en Batches")

    total_rows = len(st.session_state.df)
    st.markdown(f"üî¢ Nombre total de lignes : **{total_rows}**")

    start_line = st.number_input("üìç Ligne de d√©part (incluse)", min_value=0, max_value=total_rows - 1, value=0)
    end_line = st.number_input("üèÅ Ligne de fin (exclue)", min_value=start_line + 1, max_value=total_rows, value=min(start_line + 100, total_rows))

    batch_size = st.number_input("üì¶ Taille d‚Äôun batch", min_value=10, max_value=1000, value=100, step=10)

    selected_df = st.session_state.df.iloc[start_line:end_line].copy()
    total_selected = len(selected_df)
    total_batches = math.ceil(total_selected / batch_size)

    nb_batches_to_run = st.number_input("üöÄ Nombre de batches √† ex√©cuter", min_value=1, max_value=total_batches, value=total_batches)

    actual_batches = min(nb_batches_to_run, total_batches)
    actual_rows = min(actual_batches * batch_size, total_selected)
    st.info(f"üßÆ {actual_rows} lignes s√©lectionn√©es ‚Äì {actual_batches} batches de {batch_size}")

    # R√©cup√©rer les colonnes mapp√©es
    mapped_fields = st.session_state.mapping_config.get("fields", {})
    component_cols = {
        "postal_code": mapped_fields.get("postal_code"),
        "city": mapped_fields.get("city"),
        "governorate": mapped_fields.get("governorate")
    }

    if st.button("üöÄ Lancer le g√©ocodage sur cette plage"):
        batch_results = []

        for i in range(nb_batches_to_run):
            start = i * batch_size
            end = min((i + 1) * batch_size, total_selected)
            batch_df = selected_df.iloc[start:end].copy()

            # G√©n√©rer les components dynamiques pour chaque ligne
            components_list = []
            for _, row in batch_df.iterrows():
                comp = []
                if component_cols["postal_code"] and pd.notna(row[component_cols["postal_code"]]):
                    comp.append(f"postal_code:{row[component_cols['postal_code']]}")
                if component_cols["city"] and pd.notna(row[component_cols["city"]]):
                    comp.append(f"locality:{row[component_cols['city']]}")
                elif component_cols["governorate"] and pd.notna(row[component_cols["governorate"]]):
                    comp.append(f"administrative_area:{row[component_cols['governorate']]}")
                comp.append("country:TN")
                components_list.append("|".join(comp))

            batch_df["components"] = components_list

            with st.spinner(f"üîÑ Traitement du batch {i+1}/{nb_batches_to_run}..."):
                renamed_df = batch_df.rename(columns={v: k for k, v in mapped_fields.items()})
                enriched_batch = geocode_dataframe(renamed_df, address_column="full_address")

                #enriched_batch = geocode_dataframe(batch_df, address_column="full_address")
                batch_results.append(enriched_batch)
                st.success(f"‚úÖ Batch {i+1} trait√© !")

        st.session_state.batch_results = batch_results

        selected_enriched_df = pd.concat(batch_results, ignore_index=True)
        st.session_state.last_selected_enriched_df = selected_enriched_df

        if st.session_state.enriched_df is None:
            st.session_state.enriched_df = st.session_state.df.copy()

        for _, row in selected_enriched_df.iterrows():
            row_index = row.get("row_index", None)
            if row_index is not None:
                for col in selected_enriched_df.columns:
                    if col != "row_index":
                        st.session_state.enriched_df.at[row_index, col] = row[col]

        st.success("üéâ Tous les batches de la plage s√©lectionn√©e ont √©t√© trait√©s !")

# ========== AFFICHAGE DES TABS APR√àS TRAITEMENT ==========
if st.session_state.batch_results:
    enriched_df = st.session_state.last_selected_enriched_df
    batch_results = st.session_state.batch_results

    tabs = st.tabs(["üìä Total", "‚ùå √âchecs"] + [f"Batch {i+1}" for i in range(len(batch_results))])

    with tabs[0]:
        col1, col2 = st.columns(2)

        with col1:
            st.markdown(f"### üì¶ {len(batch_results)} batches trait√©s")
            st.markdown(f"- Taille de batch : `{batch_size}` lignes")
            st.markdown(f"- Lignes trait√©es : `{len(enriched_df)}`")
            total_success = (enriched_df["status"] == "OK").sum()
            total_failed = len(enriched_df) - total_success
            rate = round(total_success / len(enriched_df) * 100, 2)
            st.markdown(f"- Succ√®s : ‚úÖ `{total_success}`")
            st.markdown(f"- √âchecs : ‚ùå `{total_failed}`")
            st.markdown(f"- Taux de r√©ussite global : **{rate}%**")

        # Statistiques des niveaux de pr√©cision tri√©s
        with col2:
             if "precision_level" in enriched_df.columns:
                st.markdown("#### üéØ Niveaux de pr√©cision (global) :")
                precision_order = ["ROOFTOP", "RANGE_INTERPOLATED", "GEOMETRIC_CENTER", "APPROXIMATE"]
                precision_counts = enriched_df["precision_level"].value_counts().to_dict()

                for level in precision_order:
                    if level in precision_counts:
                        st.markdown(f"- `{level}` : {precision_counts[level]}")
                # Autres types non standards
                for level, count in precision_counts.items():
                    if level not in precision_order:
                        st.markdown(f"- `{level}` : {count}")

        st.dataframe(enriched_df)

    with tabs[1]:
        failed_df = enriched_df[enriched_df["status"] != "OK"]
        st.markdown(f"### ‚ùå Lignes √©chou√©es : {len(failed_df)}")
        st.dataframe(failed_df)

    for i, batch_df in enumerate(batch_results):
        with tabs[i + 2]:
            col1, col2 = st.columns(2)

            with col1:
                batch_success = (batch_df["status"] == "OK").sum()
                batch_failed = len(batch_df) - batch_success
                rate = round(batch_success / len(batch_df) * 100, 2)
                st.markdown(f"### üì¶ Batch {i+1} ‚Äì `{len(batch_df)}` lignes")
                st.markdown(f"- Succ√®s : ‚úÖ `{batch_success}`")
                st.markdown(f"- √âchecs : ‚ùå `{batch_failed}`")
                st.markdown(f"- Taux de r√©ussite : **{rate}%**")

            with col2:
                if "precision_level" in batch_df.columns:
                    precision_order = ["ROOFTOP", "RANGE_INTERPOLATED", "GEOMETRIC_CENTER", "APPROXIMATE"]
                    precision_counts = batch_df["precision_level"].value_counts().to_dict()

                    st.markdown("#### üéØ Niveaux de pr√©cision dans ce batch :")
                    for level in precision_order:
                        if level in precision_counts:
                            st.markdown(f"- `{level}` : {precision_counts[level]}")

                    for level, count in precision_counts.items():
                        if level not in precision_order:
                            st.markdown(f"- `{level}` : {count} ligne(s)")
            
            st.dataframe(batch_df)

# ========== RELANCE DES √âCHECS ==========
if st.session_state.enriched_df is not None:
    enriched_df = st.session_state.last_selected_enriched_df
    failed_df = enriched_df[enriched_df["status"] != "OK"]

    if not failed_df.empty:
        st.warning(f"‚ö†Ô∏è {len(failed_df)} lignes ont √©chou√© au g√©ocodage.")

        if st.button("üîÅ Relancer uniquement les erreurs"):
            with st.spinner("üîÑ Relance des lignes √©chou√©es (avec reformulation des adresses)..."):

                # R√©cup√©rer le mapping sauvegard√©
                mapped_fields = st.session_state.get("mapping_config", {}).get("fields", {})

                def reformat_address(row):
                    parts = []
                    for field in ["street", "postal_code", "city", "governorate", "country", "complement"]:
                        if field in mapped_fields and mapped_fields[field] in row:
                            value = row[mapped_fields[field]]
                            if pd.notna(value):
                                parts.append(str(value))
                    return ", ".join(parts)

                # Reformuler les adresses
                failed_df = failed_df.copy()
                failed_df["full_address"] = failed_df.apply(reformat_address, axis=1)

                # Nettoyage des anciennes colonnes de g√©ocodage
                geo_cols = [
                    'latitude', 'longitude', 'formatted_address',
                    'status', 'error_message', 'api_used',
                    'precision_level', 'timestamp'
                ]
                for col in geo_cols:
                    if col in failed_df.columns:
                        failed_df.drop(columns=[col], inplace=True)

                # Re-g√©ocodage complet
                #retried_df = geocode_dataframe(failed_df, address_column="full_address")

                renamed_df = failed_df.rename(columns={v: k for k, v in mapped_fields.items()})
                retried_df = geocode_dataframe(renamed_df, address_column="full_address")

                # Affichage debug
                st.markdown("### ‚úÖ R√©sultat des lignes relanc√©es")
                st.dataframe(retried_df)

                # Mise √† jour du enriched_df
                if "id" not in enriched_df.columns or "id" not in retried_df.columns:
                    st.error("‚ùå Impossible de mettre √† jour : colonne 'id' manquante.")
                else:
                    # Supprimer les lignes √©chou√©es avec les m√™mes id
                    failed_ids = retried_df["id"].unique()
                    enriched_df_cleaned = enriched_df[~enriched_df["id"].isin(failed_ids)]

                    # Fusionner avec les lignes corrig√©es
                    updated_df = pd.concat([enriched_df_cleaned, retried_df], ignore_index=True)

                    # Trier si besoin
                    updated_df = updated_df.sort_values(by="id").reset_index(drop=True)

                    # Enregistrer la mise √† jour
                    st.session_state.enriched_df = updated_df
                    st.session_state.last_selected_enriched_df = updated_df  # cl√© ici pour l'export

                    st.success(f"‚úÖ {len(retried_df)} lignes corrig√©es ! R√©sultats mis √† jour.")

# ========== EXPORT ==========
if st.session_state.last_selected_enriched_df is not None:
    if st.button("üìÖ Exporter en CSV"):
        df_export = st.session_state.last_selected_enriched_df
        df_export.to_csv("data/output/geocoded_results_google.csv", index=False)
        st.success(f"üìÅ {len(df_export)} lignes export√©es dans data/output/geocoded_results_google.csv")
